# Nueral Network와 인간의 두뇌의 비교

## 먼저 알아야 할 것은, 이 주제는 "왜 GPT는 인간의 언어를 완전히 이해 못하는가?"라는 궁금증에서 시작되었다.

### 뉴런의 개수

인간의 뇌세포 개수 = 약 1000억 개(100B), 좀 더 정확히는 860억 개(86B)이지만 비교의 편리성을 위해 100B 개로 가정. ( 실제로는 생후 36개월까지 100B로 증가, 이후로 뇌세포 생성 속도 <<< 뇌세포 삭제 속도로 인해 조금씩 줄어든다. )

이번에 발표된 LLaMA 3 - 8B 70B(거의 인간) 405B(인간의 4배) 3가지 버전이 있다.
위에 숫자는 파라미터의 개수로, 여기서 파라미터 = 퍼셉트론 = 인간의 뉴런(뇌세포) 역할을 하는 단자의 개수이다.

즉,  인간은 100B개의 뉴런으로 약 20년을 학습하여 스스로를 책임질 판단력을 가진 '어른'이 된다.
다만 인공신경망인 Neural Network(이하 NN)와 인간의 학습 속도는 인간의 뉴런이 어떤 계기로 학습을 시작하여 어떤 주기로 영향을 주는지 알 수 없으므로 비교 불가하다.

### 뉴런의 구조

인간의 경우, 뉴런은 100B 개지만, 시냅스(뉴런 - 뉴런의 연결 지점)의 개수는 약 100조 개다.
즉, 뉴런 1개당 약 1000개의 뉴런과 연결되어 신호를 주고 받는다.

NN의 경우, 아 Nueral Network와 인간의 두뇌의 비교

## 먼저 알아야 할 것은, 이 주제는 "왜 GPT는 인간의 언어를 완전히 이해 못하는가?"라는 궁금증에서 시작되었다.

### 뉴런의 개수

인간의 뇌세포 개수 = 약 1000억 개(100B), 좀 더 정확히는 860억 개(86B)이지만 비교의 편리성을 위해 100B 개로 가정. ( 실제로는 생후 36개월까지 100B로 증가, 이후로 뇌세포 생성 속도 <<< 뇌세포 삭제 속도로 인해 조금씩 줄어든다. )

이번에 발표된 LLaMA 3 - 8B 70B(거의 인간) 405B(인간의 4배) 3가지 버전이 있다.
위에 숫자는 파라미터의 개수로, 여기서 파라미터 = 퍼셉트론 = 인간의 뉴런(뇌세포) 역할을 하는 단자의 개수이다.

즉,  인간은 100B개의 뉴런으로 약 20년을 학습하여 스스로를 책임질 판단력을 가진 '어른'이 된다.
다만 인공신경망인 Neural Network(이하 NN)와 인간의 학습 속도는 인간의 뉴런이 어떤 계기로 학습을 시작하여 어떤 주기로 영향을 주는지 알 수 없으므로 비교 불가하다.

### 뉴런의 구조

인간의 경우, 뉴런은 100B 개지만, 시냅스(뉴런 - 뉴런의 연결 지점)의 개수는 약 100조 개다.
즉, 뉴런 1개당 약 1000개의 뉴런과 연결되어 신호를 주고 받는다.
또한, 인간의 뉴런은 3차원 공간에 배치되어 "전두엽의 판단"에 의해 일부 뉴런을 선택해 사용한다.

![image](https://github.com/user-attachments/assets/8d2fdba5-6276-4a68-b864-588508877121)
NN의 경우, 그림처럼 세로로 1열로 나열되어 있는 Layer의 뉴런들이, 오른쪽에 있는 다음 Layer의 뉴런(원)들에게 값를 전달한다.
이 말은 즉, NN은 2D로 이루어져 있으며, 각 Layer의 뉴런은 재사용되지 않는다.

인간의 뉴런은 3차원에서 복잡하게 시냅스를 형성하고 있는 반면,
NN의 뉴런은 1열로 나열되어 다음 Layer에게 값을 전달하는 역할을 하며, 한번 값을 보내면 다시 재사용되지 않는다.
여기서 나는 학습을 Layer -> Layer로 전송하는 구조인 Feedforward NN의 구조 자체가 시작할 때의 논제인 "GPT가 인간을 완전히 학습하지 못하는 이유"라고 결론지었다.

### 뉴런의 학습

%%% 여기서부터는 전문 의학 분야이기 때문에 설명의 세부적인 내용이 정확하지 않을 수 있다는 점을 염두에 두시길 바랍니다. 이후에 의학 계열 전문가에게 질문 후 검증된 fact로 내용을 수정할 예정입니다. %%%

인간의 학습은 의사 결정을 통해 그리고 의해 이루어진다. 그리고인간의 의사 결정의 중심체는 '전두엽'이다. 
전두엽은 목표를 설정하고 다음 행동을 결정한 후, 행동으로 인한 결과값을 통해 뉴런과 시냅스에게 보상(Reward)와 처벌(Penalty)를 준다.
전두엽은 보상으로 뉴런/뇌세포에 강한 자극을, 처벌로는 기존보다 약한 자극을 보낸다. 거듭된 처벌로 약해진 뉴런(뇌세포)나 시냅스는 결국 폐기된다. 이를 '시냅스 가소성'이라고 한다.

단기 강화 ( Short Term Potentiation ) : 일시적인 변화. 단기기억 형성. 이 강화가 반복적으로 이루어지면 Lorg-term Potentiation으로 넘어감.

장기 강화 ( LTP ) : 장기 기억. 시냅스의 강도가 오랫동안 강화됨 -> 시냅스의 물질 방출력 자체가 강화 , 새로운 기억을 형성하는 "해마"에 중요한 작용을 한다.

장기 약화 ( Long-term Depression ) : 장기적인 시냅스 약화(뉴런의 칼슘 농도 약화). 폐기까지 이를 수 있음.

### NN의 의사결정

우리가 아는 Chat-GPT같은 LLM 모델들은 전두엽 역할을 하는 의사 결정 매체가 있다. 이것을 'Policy', 즉 정책이라고 부른다.
가장 시초적인 것은 Trust Region Policy Optimization( 이하 TRPO )인데, 이것은 Transformer에 대한 논문처럼 난이도가 극악이니 생략하겠다.

내가 살펴본 것은 PPO( Proximal Policy Optimization)이다. 이 PPO는 2가지 주요 기능으로 이루어져 있다.

    1. Clipped Mechanism
  
이전 정책을 a, 새로운 정책을 b라고 하면 ( a + 0.x * b)를 통해 새로운 정책의 영향력을 조절하는 것이다. 이를 통해 학습으로 인한 변화가 급격하지 않도록 묶어둘 수 있다.
특정 범위를 제한해두고, 이 범위보다 b의 값이 높게 측정될 시 감소시켜서 적용하거나 아예 b를 0으로 만들어 해당 회차의 학습을 스킵한다.

    2. Value Function
  
현재 대부분의 LLM들은 Human Feedback - Reinforced Learning 방법으로 학습한다. 즉 인간이 준 피드백을 기준으로 학습한다는 뜻이다.
이 PPO 알고리즘에서는 다음와 같은 과정을 따른다.
1. Policy가 input에 대한 output(다음 행동)을 만든다.
2. output을 사람에게 1~5점으로 평가받는다. ( 이때 받은 것을 reward라고 한다.)
3. reward를 통해 Policy를 수정한다.
   1~3를 반복한다.

또 하나 흥미로운 점은 이 알고리즘이 State machine이라는 것이다.
따라서 이 Policy는 하나의 state(상태)를 가지며, 확률 계산에 의해 다음 state(상태 = 행동)을 선택하여 output을 만들어내고, 이 output을 사람에게 평가받는다.

이 Policy - state의 관계가 전두엽 - 뉴런의 관계와 완전히 똑같다는 생각이 들었다.

그렇다면 이 PPO 알고리즘을 chain algorithm으로 만들어서, LLM의 프롬프트로 적용한다면 어떨까?

를 주제로 연구하고 구현하고 논문을 써서 다음에 돌아오도록 하겠다 ㅎㅎ;;
